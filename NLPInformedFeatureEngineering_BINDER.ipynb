{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfcea6e-86c0-4470-beed-cde7d905b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c03ca-f64d-4109-8078-26f2fbbea818",
   "metadata": {},
   "source": [
    "# Scenario: Creating a Set of Machine Learning Friendly Features from EHR Data to Predict Diabetes Onset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42703833-311f-4f75-97db-6cee7e69c4c2",
   "metadata": {},
   "source": [
    "First we will load in the necessary data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d741e79-49e5-4388-ab9d-42ad714c9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_file(filename):\n",
    "    print(f\"Loading data for {filename}\")\n",
    "    df = pd.concat([ # use pd.concat to append/concatenate the data for all states together into a single frame\n",
    "        pd.read_parquet(f\"https://dicbworkshops.s3.amazonaws.com/{output_dir}/parquet/{filename}\") # use read_csv to load the data from each output directory\n",
    "        for output_dir in tqdm(['output_hi_small', 'output_ma_small', 'output_tx_small', 'output_wa_small']) # loop over each output directory\n",
    "    ])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37273424-1e27-4117-abdb-26f6a940b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the conditions\n",
    "conditions = load_data_for_file('conditions.parquet')\n",
    "observations = load_data_for_file('observations.parquet')\n",
    "medications = load_data_for_file('medications.parquet')\n",
    "procedures = load_data_for_file('procedures.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861f4f1-7c49-4c72-b63f-60354bd7ebc2",
   "metadata": {},
   "source": [
    "## Filtering Out Patients with Diagnoses of Type-2 Diabetes\n",
    "For this exercise, we are interested in filtering out all patients with a diagnosis of Type-2 diabetes \\\n",
    "We can filter these out based on the SNOMED code `44054006`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5fb450-c0dc-4066-8ff8-d532e716fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type2_patients = conditions.query('CODE == 44054006').sort_values(by=['PATIENT', 'START']).drop_duplicates(subset=['PATIENT', 'START'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aedca2-9fc1-4575-96fe-9da07b59594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a dictionary/lookup table to map each patient's ID to the date of their earliest Type 2 diagnosis\n",
    "patient_diagnosis_dates = {\n",
    "    row['PATIENT']: row['START']\n",
    "    for _, row in type2_patients.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71783d84-cb79-4989-ae7a-87cef0d67fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d7379-acaa-44e4-ab0c-9d2eb4198d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add simplified date columns to the observations, medications, and procedures \n",
    "observations_simplified = observations.assign(\n",
    "    DATE_SIMPLE=lambda x: pd.to_datetime(x['DATE']).dt.date.astype('str')\n",
    ")\n",
    "medications_simplified = medications.assign(\n",
    "    DATE_SIMPLE=lambda x: pd.to_datetime(x['START']).dt.date.astype('str')\n",
    ")\n",
    "procedures_simplified = procedures.assign(\n",
    "    DATE_SIMPLE=lambda x: pd.to_datetime(x['START']).dt.date.astype('str')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7eee2-f345-43dd-ba91-baf900228272",
   "metadata": {},
   "source": [
    "## Filtering Out Post-diagnosis Conditions, Observations, Medications, and Procedures and Unifying Into a Shared Representation\n",
    "First we need to filter out all EHR data from encounters that took place after the Type-2 diabetes diagnosis for the type 2 patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1c71a-390a-4e34-b6f4-3ba5dfd6ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, patients, date_column='DATE_SIMPLE'):\n",
    "    data_filtered = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        patient = row['PATIENT']\n",
    "        date = row[date_column]\n",
    "        if patient in patients and patients[patient] > date:\n",
    "            data_filtered.append(row)\n",
    "    return pd.DataFrame(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfcf363-78bc-44d5-aa85-05bce3fc9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_filtered = filter_data(conditions, patient_diagnosis_dates, 'START') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7c368-09ab-4499-b32a-cf46eaca0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_subset = observations_simplified[observations_simplified['PATIENT'].isin(patient_diagnosis_dates)]\n",
    "observations_filtered = filter_data(observations_subset, patient_diagnosis_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba00a7d-57e5-4dd9-b7aa-26f60c8381c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "medications_subset = medications_simplified[medications_simplified['PATIENT'].isin(patient_diagnosis_dates)]\n",
    "medications_filtered = filter_data(medications_subset, patient_diagnosis_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e537e-7acb-47eb-b67d-4428673c4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures_subset = procedures_simplified[procedures_simplified['PATIENT'].isin(patient_diagnosis_dates)]\n",
    "procedures_filtered = filter_data(procedures_subset, patient_diagnosis_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71e8ac-469b-4e2c-b565-a884eb3c77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28111f2-501b-4171-b91c-6fa1bc494e9b",
   "metadata": {},
   "source": [
    "Now we will label the EHR data for the type 2 and non-type 2 patients and unify everythin into a single event set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f458e0c-c7be-4060-86d1-c3e2728e2471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unified_records(conditions, observations, medications, procedures):\n",
    "    return pd.concat([\n",
    "    conditions[['PATIENT', 'START', 'CODE', 'DESCRIPTION']].assign(\n",
    "        EVENT_TYPE='CONDITION',\n",
    "    ).rename(columns={'START': 'DATE'}),\n",
    "    observations[['PATIENT', 'DATE_SIMPLE', 'CODE', 'DESCRIPTION']].assign(\n",
    "        EVENT_TYPE='OBSERVATION',\n",
    "    ).rename(columns={'DATE_SIMPLE': 'DATE'}),\n",
    "    medications[['PATIENT', 'DATE_SIMPLE', 'CODE', 'DESCRIPTION']].assign(\n",
    "        EVENT_TYPE='MEDICATION',\n",
    "    ).rename(columns={'DATE_SIMPLE': 'DATE'}),\n",
    "    procedures[['PATIENT', 'DATE_SIMPLE', 'CODE', 'DESCRIPTION']].assign(\n",
    "        EVENT_TYPE='PROCEDURE',\n",
    "    ).rename(columns={'DATE_SIMPLE': 'DATE'})\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0446879-f871-4b5b-8e9d-891731ec79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with the Type 2 diabetes patients, we label and unify the data from medications, procedures, observations, and conditions into a single recordset\n",
    "all_records_type2 = get_unified_records(conditions_filtered, observations_filtered, medications_filtered, procedures_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3376b9e-96f6-43da-b945-a6a970c7c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will do the same for the non type2 patients\n",
    "all_records_non_type2 = get_unified_records(\n",
    "    conditions[~conditions['PATIENT'].isin(type2_patients['PATIENT'])],\n",
    "    observations_simplified[~observations_simplified['PATIENT'].isin(type2_patients['PATIENT'])],\n",
    "    medications_simplified[~medications_simplified['PATIENT'].isin(type2_patients['PATIENT'])],\n",
    "    procedures_simplified[~procedures_simplified['PATIENT'].isin(type2_patients['PATIENT'])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb11a3c-e2a1-45a7-99b0-77c135eb8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records_non_type2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39dbdc4-3fcc-4374-9bb0-0c41af1bd56c",
   "metadata": {},
   "source": [
    "## Now Like Good Machine Learning Practitioners, we will split our Type 2 and Non-Type 2 patients into Training and Hold-out sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0b62e-de52-4155-b09d-79e3ee5b1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "type2_patients = all_records_type2['PATIENT'].unique()\n",
    "non_type2_patients = all_records_non_type2['PATIENT'].unique()\n",
    "labels = np.concatenate([np.ones(type2_patients.shape), np.zeros(non_type2_patients.shape)])\n",
    "train_patients, test_patients, train_labels, test_labels = train_test_split(np.concatenate([type2_patients, non_type2_patients]), labels, test_size=0.2, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b44e6c-312e-4cee-8cf7-323fb5ff3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split the records accordingly\n",
    "all_records = pd.concat([all_records_type2, all_records_non_type2])\n",
    "train_records = all_records[all_records['PATIENT'].isin(train_patients)]\n",
    "test_records = all_records[all_records['PATIENT'].isin(test_patients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4cb2c-25f5-459a-80a0-bc2aff1d6be0",
   "metadata": {},
   "source": [
    "## Option 1: Bag of Labeled Clinical Encounters\n",
    "The simplest feature representation we can create and test is a binary vector representation \\\n",
    "which encodes the occurence or lack-therof of different clinical encounters/events \\\n",
    "To construct this representation, we can use the scikit-learn package's `CountVectorizer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa149bd-b989-4495-9cfd-c7d29c4c071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3183bbc-56c5-49ec-b103-6728c73952f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    binary=True,\n",
    "    tokenizer=lambda x: x.split('|'),\n",
    "    token_pattern=None,\n",
    "    lowercase=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd03a3-b4d2-467c-be57-3e16740adf2e",
   "metadata": {},
   "source": [
    "The vectorizer expects a `'|'` pipe delimited string of coded encounters, \\\n",
    "so we will construct this representation now for our training patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9da866-1c52-4f43-9fbd-e4b2e6790fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condense the records into a pipe-delimited string of event tokens per patient, \n",
    "# where each token is of the form <EVENT_TYPE>::<CODE>\n",
    "train_records_condensed = train_records.assign(\n",
    "    EVENT_TOKEN=lambda x: x['EVENT_TYPE'] + '::' + x['CODE'].astype(str) + '|'\n",
    ").groupby(['PATIENT'])['EVENT_TOKEN'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135689e6-1265-463f-80e5-3a41cc463375",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_final = train_records_condensed.assign(\n",
    "    LABEL=lambda x: x['PATIENT'].isin(type2_patients).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72791cc3-a26f-4937-b58a-ff63e6f9a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a68d80-cbba-4293-9940-20054b528942",
   "metadata": {},
   "source": [
    "## Now that we have constructed a Pipe Delimited Event Representation, We Can Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff255e-52c5-4c5a-8650-3efd45348850",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_occurence_vectors = vectorizer.fit_transform(train_data_final['EVENT_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5732ce-c478-47b8-a6d1-ab38ccb03662",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_occurence_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7314456b-54d7-4cf4-9049-edcce62c755a",
   "metadata": {},
   "source": [
    "## Now We Will Test the performance of this representation on the Training Set Using KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694425dc-3142-40a7-beb7-eaea7f97a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1b113-00ee-4d64-83f3-27572374049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5)\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641bac58-8684-4711-846e-47fc553da437",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for i, (train_index, test_index) in tqdm(enumerate(kfold.split(event_occurence_vectors, train_data_final['LABEL'])), total=5):\n",
    "    train_x, train_y = event_occurence_vectors[train_index], train_data_final['LABEL'].to_numpy()[train_index]\n",
    "    test_x, test_y = event_occurence_vectors[test_index], train_data_final['LABEL'].to_numpy()[test_index]\n",
    "    # fit the model on the training fold\n",
    "    clf.fit(train_x, train_y)\n",
    "    # evaluate the model on the validation fold\n",
    "    preds = clf.predict(test_x)\n",
    "    scores = clf.predict_proba(test_x)[:, 1]\n",
    "    # get the AUROC\n",
    "    fpr, tpr, _ = roc_curve(test_y, scores)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    # get the confusion matrix\n",
    "    cm = confusion_matrix(test_y, preds)\n",
    "    # save the metrics\n",
    "    metrics.append({\n",
    "        'AUROC': auroc,\n",
    "        'Precision': cm[1, 1] / cm[:, 1].sum(),\n",
    "        'Recall': cm[1, 1] / cm[1].sum(),\n",
    "        'Specificity': cm[0, 0] / cm[0].sum()\n",
    "    })\n",
    "pd.DataFrame(metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814e1ea-83f8-4231-919c-0ccc48d025c0",
   "metadata": {},
   "source": [
    "## The model appears to be performing well so far, but is that where the story ends, or does the plot thicken?\n",
    "One thing that we can do is audit the way our current model is behaving, and what its predictions are based \\\n",
    "on, by looking at feature importance rankings. Here we inspect the top 50 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2e7e2-2d72-4f46-8feb-20376cd2c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_lookup = {\n",
    "    value: key for key, value in vectorizer.vocabulary_.items()\n",
    "}\n",
    "top_50_indices = np.argsort(clf.feature_importances_)[-50:]\n",
    "top50_features = [reverse_lookup[idx] for idx in top_50_indices]\n",
    "top50_importances = clf.feature_importances_[top_50_indices]\n",
    "top_features_df = pd.DataFrame({\n",
    "    'FEATURE_NAME': top50_features,\n",
    "    'FEATURE_IMPORTANCE': top50_importances\n",
    "}).assign(\n",
    "    CODE=lambda x: x['FEATURE_NAME'].str.split('::').apply(lambda pair: pair[1])\n",
    ").merge(\n",
    "    all_records[['CODE', 'DESCRIPTION']].drop_duplicates().astype({'CODE': str}),\n",
    "    on='CODE',\n",
    ").sort_values(by='FEATURE_IMPORTANCE', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b22e4-3eb7-49a8-a361-6f818cb216ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2716e-dd0a-47e4-a9df-6ce4ff85abcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
